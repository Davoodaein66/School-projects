# Final Project.docx
Overview
The project consists of several key stages: data collection, exploratory data analysis (EDA), feature engineering, model development, and model evaluation. Data collection involves gathering historical loan data from the bank's databases, including customer profiles, financial history, and campaign response statistics. EDA helps identify patterns, correlations, and potential outliers in the dataset. Feature engineering involves creating new features or improving existing ones while maintaining robustness by removing outliers and missing values. Model development focuses on training machine learning models such as logistic regression, decision trees, K-Nearest Neighbors (KNN), and Random Forest to predict loan eligibility. Model evaluation assesses performance using metrics like accuracy, precision, recall, and F1-score.

Methodology
The methodology follows a systematic approach that includes data preparation, model selection, model optimization, and model evaluation. Data preparation involves splitting the dataset into training and testing sets, and defining feature matrix X and target variable y. Model selection includes evaluating multiple machine learning algorithms to determine the best model for predicting loan eligibility. Model optimization involves fine-tuning model hyperparameters using techniques such as cross-validation and grid search. Model evaluation uses metrics such as accuracy, recall, and F1-score to assess model performance.

Models
The project considers four machine learning models: logistic regression, K-Nearest Neighbors (KNN), decision trees, and Random Forest. Logistic regression is a widely used statistical model for classification tasks. KNN is a proximity-based prediction algorithm. Decision trees are a classification technique that uses attribute variables to group new occurrences into predetermined classes. Random Forest is an ensemble learning method that combines multiple decision trees for improved performance.

Results
Analysis showed that the Random Forest model outperformed the other models in terms of accuracy and recall. Logistic regression achieved high accuracy but moderate recall. KNN showed encouraging accuracy and recall, with optimal performance under specific parameters. Decision trees achieved high precision and recall, with post-pruning techniques improving performance. Random Forest demonstrated the best overall performance with high accuracy and recall.

Conclusion
The Random Forest model demonstrated the best balance between accuracy and recall, making it the most efficient approach for predicting personal loan eligibility. Iterative model evaluation and optimization emphasize the importance of continuous improvement in predictive analytics.

# Final Project Applied Data Mining 
Related codes for the above project

# Final Paper-Team 8.pdf
This project focuses on using advanced data mining and machine learning techniques to predict house prices in the real estate market. By leveraging a comprehensive dataset that includes both numerical and categorical features related to properties, such as location, size, and quality, the project explores the factors that influence house prices. The goal is to develop accurate predictive models that can assist stakeholders such as property developers, buyers, and sellers in making strategic decisions, optimizing pricing, and gaining insights into market trends.

Background
House prices play a critical role in personal wealth, homeownership accessibility, and economic indicators at large. Understanding the connection between high housing prices and their impact on spending habits and the macroeconomic environment is essential. This project investigates the dynamics of elevated house prices and their effects on different aspects of the economy, including consumer confidence and investment decisions.

Methodology
The project follows a systematic approach that includes data collection, data preprocessing, exploratory data analysis, model development, and model evaluation. The dataset is sourced from various real estate-related files, providing a rich set of data for analysis. Data preprocessing involves cleaning and transforming the data to improve model performance. Model development includes utilizing various machine learning models such as linear regression, decision trees, and ensemble methods like Random Forest. Model evaluation is based on metrics such as accuracy, mean absolute error (MAE), and root mean square error (RMSE) to assess performance.

Project Goals
The primary goal of this project is to build predictive models that estimate house prices accurately based on different features. This can lead to better decision-making and resource allocation in the real estate sector. The project also aims to explore the advantages and limitations of traditional real estate valuation practices and modern data-driven methodologies, ultimately aiming to enhance the overall efficiency and effectiveness of house price prediction.

Results
The project's analysis revealed that different machine learning models perform well in predicting house prices, with varying levels of accuracy and reliability. Ensemble methods, such as Random Forest, often provide the best balance between performance and interpretability. These findings offer insights into which models are most effective for predicting house prices in various scenarios.






# Research Project-1.pdf,  Modeling Tailpipe CO2 Emissions and Petroleum Consumption
This project aims to statistically investigate the relationship between tailpipe carbon dioxide (CO2) emissions and annual primary-fuel petroleum consumption in vehicles. By focusing on key vehicle characteristics such as fuel economy, engine displacement, and combined luggage and passenger volume, the analysis seeks to develop a model that accurately predicts CO2 emissions and assists automobile manufacturers in designing vehicles that minimize their carbon footprint.

The research utilizes a large dataset of 43,177 vehicle models from 1984 to 2021, providing extensive insights into fuel types, vehicle make and model, transmission type, and more. The data has been sourced from FuelEconomy.gov and includes information on various characteristics of each vehicle, such as engine size and the number of cylinders. By examining these variables in relation to tailpipe CO2 emissions, the project aims to uncover patterns and trends that can guide more efficient and environmentally friendly vehicle design.

The analysis uses multiple linear regression to model CO2 emissions based on critical vehicle characteristics, including annual primary-fuel petroleum consumption, combined miles-per-gallon, engine displacement, and combined luggage and passenger volume. Bivariate analysis shows that regular gasoline has a significant impact on CO2 emissions, while unknown vehicle types contribute the most among all categories. Despite some level of multicollinearity, the final regression model effectively explains the variance in tailpipe CO2 emissions.

The project leverages tools such as MATLAB, Microsoft Excel, and SAS Enterprise Guide for data analysis and visualization. Statistical methods include probability density functions, Pearson correlation coefficients, and chi-squared testing to evaluate relationships between variables and assess the model's accuracy. The results indicate a strong correlation between CO2 emissions and annual petroleum consumption, supporting the hypothesis that reducing petroleum consumption can lead to decreased CO2 emissions.

While the study presents valuable findings, there are limitations to consider, including missing data and potential biases in the sample set. Future research could address these issues by refining the dataset and exploring additional methods to improve model precision. Nevertheless, the project's insights contribute to a better understanding of the factors influencing tailpipe CO2 emissions and offer potential pathways for reducing the environmental impact of vehicles.

















